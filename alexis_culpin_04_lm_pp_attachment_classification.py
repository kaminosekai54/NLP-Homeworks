# -*- coding: utf-8 -*-
"""04_LM_PP_Attachment_classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks/04_LM_PP_Attachment_classification.ipynb

# PP Attachment

The Preposition Phrase attachment problem is the difficulty to decide if a preposition in a sentence is attached to a verb or a noun.

In some cases it may even confuse the reader. For example, the sentence:

> San Jose cops kill man with a knife

can be interpreted as either the man or the cops had a knife. The difference in the syntax parsing would be attaching the preposition phrase either to the Verb Phrase ("kill with knife")

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANcAAACICAIAAAD7x+nAAAAPoElEQVR4nO2df2wT5R/Hn34zBRTkl4oOZ4tgxATwx5yyzrh26jp12S6SjIqyTQyChoUlYptgsk2jxFaMMxC3Sdw60FFn0FsC28rEdYaVsIk4u+AUtQdjM5CB6wgbhOF9/3jKrbS3/rhe77nrPq8s5No+9zyfe/rm8zzP3fV9KpZlEQAQ5X+kAwAAUCEgA0CFAHlAhQB5QIUAeUCFAHlAhQB5QIUAeUCFAHlAhcJhGFRcjDQapFIhnQ45naQDUiygQuFgCTqdiGVRRQWqqCAdkGJRwXVkwahUyONBGg3pOJRPEukAFEx5OdLpkEaDdDrfHyAMyIUxwTCIYZDTiWw2VFwMg7JAQIXiwDBo0SIEfSkMWJ0Ih1sXMwyy2ZBaTTogxQLzQuHgdXFHB1KrfYtlQBgwIgPkgREZIA+oECAPqBAgD6gQIA+oUDg9PT1VVVV6vb6yspJhGNLhKBhYI0fHlStXXC7XkSNHXC7X4sWLtVqtVqs9duwYTdNz586lKCozM5N0jMoDVBgR//zzD1ae2+3WarXp6elarXbmzJn+ZY4fP07TtNvtpiiKoqjbbruNVLSKA1QYihMnTrhcLpfLNTY2hpX32GOPhd7lwoULNE3TNJ2WlkZR1PLly6UJVdGACgP577//sPKOHDly99134zH3vvvui7aeQ4cO0TQ9NjZGUVRubm48Qk0YQIU+hoaGsPKOHj2KlZeenj537twYq/3zzz9pmnY4HHiYXrhwoSjRJhhTXYUnT57Eme/8+fNYeenp6aK3cvXqVTxML1y4kKIorVYrehOKZoqq8OjRo1h8c+bMwZnvgQcekKDdrq4umqb/+usvnBpnzJghQaPyZwqp0Ov1chO+1NRULL477rhD+kjOnj2LU2NmZiZFUUuXLpU+BlmR+Cr0eDz4JEt/fz834UtKksUtba2trTRNI4QoisrJySEdDjESVoU///wzznzTpk3DJ1mWLVtGOih++vr6aJru6OjAw/SCBQtIRyQ1CaXC0dFRbsx98MEHceZLTk4mHVdEjI2N4WF68eLFFEU9/vjjpCOSjkRQ4ZkzZ7Dyfv/9d27MVe7E3+Vy0TQ9MDCAU+NNN91EOqK4o2AVut1unPmuXbuGlffII4+QDko0BgYGcGo0GAwURS1ZsoR0RHFEYSq8evWq6zoajQZnPnVC/+5o//79NE3PmDGDoqinn36adDhxQRkqPHv2LFbeL7/8or3OrFmzSMclHW63m6bp7u5uPEzPmzePdERiImsV/vbbb/gky8WLF7Hy0tLSSAdFkpGRETxML1++nKKohJmByFGF3Ji7YMECfJIlsWdFAujo6KBp+t9//8WpkXQ4sSI7Fer1eqw8rVabYOOO6DAMQ9N0U1NTe3s76VhiQnYqBKYg8LsTgDygQoA8oEKAPMRUqFKh4uIbXnIb+A+cogNgGKTRIP9fnHLvKL3TSOZCp5O/y1gWsSyYUgaCTWNttol3bDafmSxSeKcRWyOrVKiuDtlsPiGqVD4LSm4jYBtACDEM0ukm0iG2q8MPGVB0p5HMhcXFPjteXmw2BD8wD0CjmTBKxPoL8HZXaqexhMAtt7ezavXES7yB/zIz2fZ2UtHJl/Z2NjOTZdkb+kfpnUb4xnc8rfGf6yDEM6DgtYviBpp4gJ8k4HT6RmcORXcO+Z9fVFTcsFjmhWUnFtFAcTHS61FdHek4xIPk6oRrubgY1dfzrE4mKw8EnJFReuco5jqy0jsaCIEyrp3g4RgG5USF/LwwEiALJjbKyIVAYgMqBMgjIxV6vd6amhq9Xl9TU+P1ekmHowBGRka++eYbvV7f0tJCOpaYkMUa+fTp03a7vbOz02g0Go1Gu91ut9szMjKMRuO9995LOjo54nQ6Dx482NvbO3/+/NTU1EuXLg0ODppMJoX6IxJWodvtttvt/f39RqPx+eef9/+oubnZbrenpKQYjUbw5cWcOHHC4XA4HI6VK1dmZ2cfOHAgIyMD91tPT4/Vas3KynrttddIhxk1xFTY2dlpt9sRQkajMSMjI8Ziic3Q0BAW38yZMw0Gg8FgmD59eklJSXCf7Nmzp6mpyWw2K+snswRUKCDJhUiZiU1bW5vD4WAYBouPm58UFha+/fbbvL03NDRksVhmzZplNpunTZsmbbxCkfLWiYaGhry8PIvFcurUKQG7nzp1ymKx5OXlNTQ0iB6brDh+/PiHH36o1+vff//97u7ugE/z8vLCduChQ4eys7O/++67uMUoJlKocHh4uLq6WqfTVVdXDw8Py6o2WTEwMFBbW7t69erNmzc3Nzdfu3YtoMDw8LBOp4v8qD/55JMNGzb8/fffYkcqMvEdkf0Xvy+99JK4le/duzcxltLj4+N42jc0NIRH3rvuuiu42OnTp0tKSpqamqKqvK+vz2KxpKWlvfnmmyLFKz7xUqFkMzlFL6W7urocDkdHRwcW34oVKyYr6Xa7P/roo927dwtrqLGx8csvvzSZTE8++aTQYOOI+CoksqpV1lKaYRic/JYsWWIwGML6weGj27FjRyyNjoyMWK1WhJDJZJLb09HEVCHxtCTzpfTo6CgW3+XLl3Hyi8SIp7m5ubOz84MPPhAlhsOHD1ut1ldeeaWgoECUCkVBHBXKaooW18moMA4fPnzw4MGffvoJiy/yR0vs3bu3v7/fZDKJG89nn33W3d1tNptl8pCLmFTo9Xrx1TZ85W327NkiRhYjcojtjz/+wMnv4Ycfzs7Ofuqpp6LavaamBiG0YcOGeMTm8XgsFsvSpUtLS0vjUX9UCFShDPPNZEifp4eHh7H4kpKScPILeIZtJFit1pSUlHj3LU3TVVVVZrM5Kysrrg2FJmoVynzuNRnSzFnb29sdDkdfXx8Wn4AniWLeeecd7gJxvLly5YrFYrl48aLZbL799tslaDGYKFSorHUoL3E6hN7eXpz8MjIyDAbDypUrY6mN9wJxvOnu7rZYLPn5+WvXrpWyXUxEKiS++BUXsdL5uXPnsPjmzJmDk9/NN98cY2whLhBLwBdffPHDDz+YTKaHHnpIynbDqFBWi19xiWVqi8V35swZLL577rlHlJDy8/N37NhBtp8HBgasVmtycrLZbJas0VAq1Ov1ksVBnMitofV6fXZ2tsFgePTRR0UMQK/X0zQtk/MMLS0tVqtVMrtsWdxrDUxxZPS7E2DKAioEyAMqBMgTToUMg4qLfXahMZomK9PgI4SbtADk7ObNG4M0gYVTIZag04lYFlVUKNI0OTZCu0kLQM5u3rwxSBFYmHuxEWI9nkk/QohVq2+wFC0vD3zTvzzG42EzM33Oo1zldXWsWu3bt64usCRvhdynoWvmogrb3CR4PD7DWYxaPWmXhAUhtq7OZ8bK3uhg61+GCLwxSBNYuIrLy1m1ms3MZMvL+b1qOU9g9vr3zV7/jgObut4Wrg1XXlTke5PTmccz8WZRkU8i/q1wFBX56uHgrZmLqrx84vvnbW5yOKdeztBXGLgPuMaDv2x/jUoMbwzSBBaBvD0etr3dJ0fuW8ey4PyUfZWF/I/jXwwnE/8kgyVSVHSD1v2b4K0wICnx1uz/JlcJb3OTw+smLQA5u3nzxiBNYNEkWf9vMWAgDthgo1chfon/x/knyBCDXywq5G0uJPhrCM7IUeE/HtTV8fecf2GEIoxOBCaLQQLCrU64pRHDIJsNqdWBBcJ6UgeTmemb7eN5vn89eA7Mzd51OlRRgRjGt1QPoKgo0Jedt2b8Et34GAbe5kKC3aTFmqFHstJjWeTxoPp6cVqUNWFUiociPIsPnt3jMTraXBjV6qSoyNdQ8BpCqtUJR+wTI/9eKSoKkwvZ69NyaSCYC6W6jmyzTTznSWIU64hdUeHrM8FnhZSCJCpUqZBaHThKSoZiVcid01Zm+FEA99QA5IHryAB5QIUAP1IOkqDCqInTLehyu7M9Kyvrxx9/lKYtUCEQiMvleuaZZ8rKyr7//nuxnElCo4yn7gCSYbFYvF5va2trUlKSXq9va2vLycl59913n3jiifg1CrkQ8HHs2LEXXnhhxYoV27ZtS0rypadnn32Wpulvv/12+/bt8WsaVAgghFBlZWVDQ0NjY+Nzzz0X8NH06dOxo01+fn5PT088WgcVTnV6e3tXrVql0Wg+/vjjW2+9dbJiubm5u3fvrq2tjdFGkRdQ4ZSmqqqqurr6888/pygqbOHZs2d/+umnycnJBQUFfX19IoYBKpyinDx5cs2aNfPmzdu5c+f8+fMj33HVqlU7d+6srKzctWuXWMGACqcitbW1Vqt1+/btq1evFrD7nXfeWV1dfcstt6xdu9bj8cQeD6hwatHf3//qq68mJSXt2rUrOTk5lqpefvnlbdu2vffee4I93zlAhVOIr776auvWrWVlZYWFhaJUmJKSUldXNz4+vn79+sHBQcH1gAqnBOfOndu4cePo6OiePXsWLVokbuXr1q0zmUxbtmz5+uuvhdUAKkx89u3bt2nTptLS0vXr18epifvvv7+hoeHChQubNm06f/58tLuDChMZr9e7efPmwcHBxsZGCez833jjjY0bN77++us0TUe1I6gwYdm/f39hYeG6detKSkoka3TZsmX79u1jGOatt966dOlShHuBChOQy5cvm83mvr6+pqYmib2BMaWlpWvWrCkoKGhpaYmkPKgw0Whra6Mo6sUXX9yyZQvBMFJTUw8cOPDrr79u3bp1fHw8dGFQYeLAsmxZWVlXV1dra2tcb8SKHLPZnJubm5OTE9qcGH79FDV6vT4ehs+xVyu3u7WDmewAQYUAeWBEBsgDKgTIAyoMRwgjZH9DYIUSOvJoj8tm85lPRxsFzAvDEGwwwr0TvCFneKMNHXmEx8UV02iE+cCACsMR4ptQlgo5RFeh4PLXgRE5MhhmwmRd2PirUqGKCqRS+RJGRQXSaHzO9f5lcAF/j31ur7COZ06nLw8xDFKpfI8h4J5HgMPm/uWOImz9AcceXJ63ZrxXhJFLYU+naLAVbIDL7mQboevhTL9DG4BHbhXO2wrL+vwa8Y6how1rRR587MHleWsO7Uke0E74A5viBDttClZh6G0BVuHBYA9m7G6KXZ8jf6AAr/HpZMfOW6H/p6E9yQPaCf0xwCLk04e/OexkG6HrCb0twCo8mKIin/strtD/MQjCVDjZsfNWGKDCiB/IAfPCCMD22jqdwAc+RYsAq3AOnQ7V10+4dtfX8xvBRn4ggo89tCd5ABGqderCdRGeCfk/KEDcXCjAKjwYHJ7/Mw2C0xiuH7+M0Io8+NjZcLkwtCf5jcCZGoA8MCID5AEVAuQBFQLkARUC5AEVAuQBFQLkARUC5AEVAuQBFQLk+T/d6c6L6IgNywAAAABJRU5ErkJggg==)

or to the Noun Phrase ("man with knife").

![image.png](data:image/png;base64,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)

Let's try out the Preposition Phrase attachment classification!

Through this exercise, you'll practice classification of linguistic aspects of text.

# Setup
Loading the data
"""

import csv

from tqdm.notebook import tqdm
from typing import Dict, List, Tuple
from random import choice
from urllib.request import urlopen

def read_pp_examples(file_url: str) -> List[Dict]:
  """Reads the pp samples from a remtoe url and loads them into a dictionary

  Args:
      file_url (str): a url to load the dataset from

  Returns:
      Dict: a dictionary with two keys: answer and pp
  """
  pp_examples = []
  
  for line in tqdm(urlopen(file_url)):
    line = line.decode("utf-8").strip().split()
    assert(len(line) == 5)
    v,n1,p,n2,answer = line
    pp_examples.append( {'answer':answer,'keywords':(v,n1,p,n2)} )
  return pp_examples

pp_samples_url = 'https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/pp_examples.txt'

pp_examples = read_pp_examples(pp_samples_url)

"""# Step #1 - Look at the data

Step 1 is (always) to examine the data!

That means to check the data statistics, load some sample at random and ensure it is correctly labeled, and if possible, plot and visualize the data (histograms, distribution, etc.). 
"""

print(f"There are {len(pp_examples)} samples in the dataset")

print(choice(pp_examples))

"""Of course, we can reach the dictionary's parts by specifying the key in a squared brackets. """

random_example = choice(pp_examples)
print(random_example['keywords'])

print(random_example['answer'])

"""# Step 2: Deciding on the measurement"""

# we can split the dataset simply through dividing the list 

amt = int(0.75 * len(pp_examples))
train_examples, test_examples = pp_examples[:amt], pp_examples[amt:]

print(len(train_examples), len(test_examples))

"""We'll define a classifier evaluator.

Given a set of examples and an evaluator, it returns the accuracy score
"""

def evaluate_classifier(examples, pp_resolver):
  """evaluate the classifier and returns the accuracy score.

  Args:
      examples (List): a list of {'keywords':(v,n1,p,n2), 'answer':answer }
      pp_resolver (_type_): a model with a classify() function that maps from 
        (v,n1,p,n2) to 'N' / 'V'

  Returns:
      float: The accurcy score of the classifier
  """
  correct = 0.0
  incorrect = 0.0
  for example in examples:
      answer = pp_resolver.classify(example['keywords'])
      if answer == example['answer']:
          correct += 1
      else:
          incorrect += 1
  return correct / (correct + incorrect)

"""# Classifiers

Let's test it on an extremely naive classifiers:
"""

class AlwaysSayN:
    """
    This naive clasifier answers always with 'Noun'
    """
    def __init__(self): pass
    def classify(self, pp):
        return 'N'

class AlwaysSayV:
    """
    This naive clasifier answers always with 'Verb'
    """
    def __init__(self): pass
    def classify(self, pp):
        return 'V'

print(evaluate_classifier(test_examples, AlwaysSayV()))

print(evaluate_classifier(test_examples, AlwaysSayN()))

"""We can see that saying always 'Noun', leads to an accuracy result of 53%.

---



It also means that our dataset is quite balaneced ;)

We could, instead, have tested which class has the majority and simply select it:
"""

class MajorityClassResolver:
  def __init__(self, training_examples: List):
    """Initializes the class, testing which class is the majority and saves it as a property.

    Args:
        training_examples (List): A list of dictionary training examples.
    """
    answers = [item['answer'] for item in training_examples]
    num_n = len([a for a in answers if a == 'N'])
    num_v = len([a for a in answers if a == 'V'])
    if num_v > num_n:
        self.answer = 'V'
    else:
        self.answer = 'N'

  def classify(self, pp: Tuple) -> str:
    """classify a 4 keywords tuple as N or V attachment, based on the previously calculated majority class

    Args:
        pp (Tuple): a tuple of V, N1, PP, N2 to be classified

    Returns:
        str: the prediction - N or V
    """
    return self.answer

print(evaluate_classifier(test_examples, MajorityClassResolver(train_examples)))

"""Or make it a bit more sophisticated by peeking at the training examples:"""

class LookupResolver:
  def __init__(self, training_examples: List):
    """Initializes the class, load all the training dataset into the memory and during prediction, return the answer if the keywords match a previously saved one. 

    Args:
        training_examples (List): _description_
    """
    self.answers = {}
    for item in training_examples:
        self.answers[item['keywords']] = item['answer']
    self.backoff = MajorityClassResolver(training_examples)
      
  def classify(self, pp: Tuple) -> str:
    """Classify a 4 keywords tuple as N or V attachment.
     If the tuple was found in the previously stored answers, return it.
     Otherwise, return the majority class.

    Args:
        pp (Tuple): a tuple of V, N1, PP, N2 to be classified

    Returns:
        str: the prediction - N or V
    """
    if pp in self.answers:
        return self.answers[pp]
    else:
        return self.backoff.classify(pp)

# If you want to understand what is stored in the `answers` property, uncomment and run the following line:
# print(LookupResolver(train_examples).answers)

print(evaluate_classifier(test_examples, LookupResolver(train_examples)))

"""# Exercise - Your Turn:

Implement a discriminative PP-attachment model, using a classifier of your choice (i.e. - Naive Bayes Classifier https://web.stanford.edu/~jurafsky/slp3/4.pdf, [Support Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), etc.) from a toolkit such as [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB).

Possible features:

Single items ​
* Identity of v ​
* Identity of p ​
* Identity of n1 ​
* Identity of n2 ​

Pairs:​
* dentity of (v, p) ​
* Identity of (n1, p) ​
* Identity of (p, n1)​

Triplets:​
* Identity of (v, n1, p)​
* Identity of (v, p, n2) ​
* Identity of (n1, p, n2) ​

Quadruple:​
* Identity of (v, n1, p, n2)​


Corpus Level:​

* Have we seen the (v, p) pair in a 5-word window in a big corpus?​
* Have we seen the (n1, p) pair in a 5-word window in a big corpus? ​
* Have we seen the (n1, p, n2) triplet in a 5-word window in a big corpus?​
*  Also: we can use counts, or binned counts.​

Distance:​
* Distance (in words) between v and p ​
* Distance (in words) between n1 and p​

For the corpus level features, you can simply load one of the english corpuses in NLTK (such as [brown - but don't forget to download it first](https://www.nltk.org/book/ch02.html)). If you use a tagged corpus - such that the words are tagged by their parts of speech, you can create more interesting linguistic-related features.

You can also consider using [WordNet](https://wordnet.princeton.edu/), a large *lexical* database of English words, which is also [implemented in NLTK](https://www.nltk.org/howto/wordnet.html). 

Finally, if you need inspiration and ideas, here are some selected papers:
- [An Analysis of Prepositional-Phrase Attachment Disambiguation](http://ejournals.asia/stj1/ijclr2.pdf) - an overview of different papers dealing with this problem using different methods (kNN, SVN, etc.).
- [The spy saw a cop with a telescope: Who has the telescope?](https://www.semanticscholar.org/paper/The-spy-saw-a-cop-with-a-telescope%3A-Who-has-the-Yan-Nguyen/3220ad0619b72404cb9b1acb9e093a8a564f0f4e) - An linguistic analysis of the potentials reasons (and features) to disambiguify pp attachment. 
- [PREPOSITIONAL PHRASE ATTACHMENT AMBIGUITY RESOLUTION USING SEMANTIC HIERARCHIES](https://eprints.mdx.ac.uk/2471/1/ppattachhier.pdf)
- [Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary](https://aclanthology.org/W97-0109.pdf)
"""

from sklearn.naive_bayes import GaussianNB

class NaiveBayesClassifier():
  
  def __init__(self, training_examples: List):
    classifier = GaussianNB()
    
  def classify(self, keywords: Tuple):
    pass

print(evaluate_classifier(test_examples, NaiveBayesClassifier(train_examples)))

import nltk
from nltk.corpus import wordnet as wn
import pandas as pd
import numpy as np
from nltk.corpus import brown
from gensim.utils import n2cp
from gensim import corpora
from sklearn.svm import SVC
from nltk.stem.snowball import SnowballStemmer
from sklearn import preprocessing
stemmer = SnowballStemmer("english")
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('brown')
# nltk.download('wordnet')
# nltk.download('universal_tagset')
from sklearn import metrics

five_word_windo_list=[]
for sentence in tagged_sent:
  # print(sentence)
  for i in range(int(np.floor(len(sentence)/5))):
    window = sentence[i*5:5*(i+1)]
    five_word_windo_list.append(window)
print('Corpus size using 5-words windows:',len(word_window))
five_word_windo_list[:3]


# In[ ]:



def find_all_indexes(lst, value):
    indexes = []
    i = 0
    while True:
        try:
            i = lst.index(value, i)
            indexes.append(i)
            i += 1
        except ValueError:
            break
    return indexes

v_p_pairs_in_corpus=[]
n1_p_pairs_in_corpus=[]
n1_p_n2_pairs_in_corpus=[]
p_n2_pairs_in_corpus=[]
for window_i in word_window:
  words = [word_type[0] for word_type in window_i]
  types = [word_type[1][:2] for word_type in window_i]
  VB_indexes = find_all_indexes(types,'VB')
  IN_indexes = find_all_indexes(types,'IN')
  NN_indexes = find_all_indexes(types,'NN')

  if 'IN' in types:
    if 'VB' in types:
      if VB_indexes[0] < IN_indexes[-1]:
        useful_words=[words[VB_indexes[0]],words[IN_indexes[-1]]]
        useful_words = [stemmer.stem(word_i) for word_i in useful_words]
        v_p_pairs_in_corpus.append(useful_words)
    elif 'NN' in types:
      if len(NN_indexes)==1:
        if NN_indexes[0] < IN_indexes[-1]:
          useful_words=[words[NN_indexes[0]],words[IN_indexes[-1]]]
          useful_words = [stemmer.stem(word_i) for word_i in useful_words]
          n1_p_pairs_in_corpus.append(useful_words)
        else:
          useful_words=[words[NN_indexes[0]],words[IN_indexes[-1]]]
          useful_words = [stemmer.stem(word_i) for word_i in useful_words]
          p_n2_pairs_in_corpus.append(useful_words)
      elif len(NN_indexes)==2:
        if NN_indexes[0] < IN_indexes[0]:
          if NN_indexes[1] > IN_indexes[0]:
            useful_words=[words[NN_indexes[0]],words[IN_indexes[0]],words[NN_indexes[1]]]
            useful_words = [stemmer.stem(word_i) for word_i in useful_words]
            n1_p_n2_pairs_in_corpus.append(useful_words)


# print(v_p_pairs_in_corpus)
# print(n1_p_pairs_in_corpus)
# print(p_n2_pairs_in_corpus)
# print(n1_p_n2_pairs_in_corpus)
print(len(v_p_pairs_in_corpus))
print(len(n1_p_pairs_in_corpus))
print(len(p_n2_pairs_in_corpus))
print(len(n1_p_n2_pairs_in_corpus))
# X_train, X_test, y_train, y_test = train_test_split(transformed_df.drop(columns="answer"), transformed_df.answer, test_size=0.3,random_state=109) # 70% training and 30% test

# clf = svm.SVC(kernel='linear') # Linear Kernel
# clf.fit(X_train, y_train)
# y_pred = clf.predict(X_test)
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
class SVC2Classifier():

  def __init__(self, training_examples: List, v_p_pairs_in_corpus,n1_p_pairs_in_corpus,p_n2_pairs_in_corpus,n1_p_n2_pairs_in_corpus):
    self.v_p_pairs_in_corpus,self.n1_p_pairs_in_corpus,self.p_n2_pairs_in_corpus,self.n1_p_n2_pairs_in_corpus = v_p_pairs_in_corpus,n1_p_pairs_in_corpus,p_n2_pairs_in_corpus,n1_p_n2_pairs_in_corpus
    #Single terms
    verbs = [stemmer.stem(example_i['keywords'][0]) for example_i in training_examples]
    # print(verbs)
    self.verbs_dict = self.get_filtered_words(verbs)
    n1 = [stemmer.stem(example_i['keywords'][1]) for example_i in training_examples]
    self.n1_dict = self.get_filtered_words(n1)
    p = [stemmer.stem(example_i['keywords'][2]) for example_i in training_examples]
    self.p_dict = self.get_filtered_words(p)
    n2 = [stemmer.stem(example_i['keywords'][3]) for example_i in training_examples]
    self.n2_dict = self.get_filtered_words(n2)
    # Construct one-hot feature vector based on whether it's identical
    single_v_features = self.one_hot_encode_identity(verbs,self.verbs_dict)
    single_n1_features = self.one_hot_encode_identity(n1,self.n1_dict)
    single_p_features = self.one_hot_encode_identity(p,self.p_dict)
    single_n2_features = self.one_hot_encode_identity(n2,self.n2_dict)

    v_p_pairs_in_corpus_count = []
    v_p_pair = [[v_i,p_i] for v_i,p_i in zip(verbs,p)]
    n1_p_pairs_in_corpus_count = []
    n1_p_pair = [[n1_i,p_i] for n1_i,p_i in zip(n1,p)]
    p_n2_pairs_in_corpus_count = []
    p_n2_pair = [[p_i,n2_i] for p_i,n2_i in zip(p,n2)]
    n1_p_n2_pairs_in_corpus_count = []
    n1_p_n2_pair = [[n1_i,p_i,n2_i] for n1_i,p_i,n2_i in zip(n1,p,n2)]
    for i in range(len(training_examples)):
      v_p_pairs_in_corpus_count.append(self.get_large_corpus_count(v_p_pair[i],self.v_p_pairs_in_corpus))
      n1_p_pairs_in_corpus_count.append(self.get_large_corpus_count(n1_p_pair[i],self.n1_p_pairs_in_corpus))
      p_n2_pairs_in_corpus_count.append(self.get_large_corpus_count(p_n2_pair[i],self.p_n2_pairs_in_corpus))
      n1_p_n2_pairs_in_corpus_count.append(self.get_large_corpus_count(n1_p_n2_pair[i],self.n1_p_n2_pairs_in_corpus))
    # print(v_p_pairs_in_corpus_count)
    # print(n1_p_pairs_in_corpus_count)
    # print(p_n2_pairs_in_corpus_count)
    # print(n1_p_n2_pairs_in_corpus_count)

    # print('train first feature dimension',np.shape(single_v_features))
    # print('train fourth feature dimension',np.shape(np.array(v_p_pairs_in_corpus_count)))
    # print(np.shape(np.array(v_p_pairs_in_corpus_count).reshape((len(v_p_pairs_in_corpus_count),1))))

    features = np.hstack((single_v_features,single_n1_features,single_p_features,single_n2_features
                          ,np.array(v_p_pairs_in_corpus_count).reshape((len(v_p_pairs_in_corpus_count),1)),np.array(n1_p_pairs_in_corpus_count).reshape((len(n1_p_pairs_in_corpus_count),1))
                          ,np.array(p_n2_pairs_in_corpus_count).reshape((len(p_n2_pairs_in_corpus_count),1)),np.array(n1_p_n2_pairs_in_corpus_count).reshape((len(n1_p_n2_pairs_in_corpus_count),1))))
    min_max_scaler = preprocessing.MinMaxScaler()
    features = min_max_scaler.fit_transform(features)
    self.min_max_scaler = min_max_scaler
    print('Feature Dimension:',np.shape(features))
    Y = [example_i['answer'] for example_i in training_examples]
    Y = [1 if answer_i == 'V' else 0 for answer_i in Y]
    classifier = SVC()
    classifier.fit(features,Y)
    self.classifier = classifier
  
  
  def get_large_corpus_count(self,target_sen, corpus):
    count = 0
    for i in corpus:
      if target_sen == i:
        count+=1
    count = count
    return count
    

  def get_filtered_words(self,words):
    stemmed_words = [[verb] for verb in words]
    dictionary = corpora.Dictionary(stemmed_words)
    filter_tokens_if_container_documents_are_less_than = 50 # Filter rare words to reduce feature num, need to be fine-tuned
    filter_tokens_if_appeared_percentage_more_than = 0.8
    keep_the_first_n_tokens=500
    dictionary.filter_extremes(
        no_below=filter_tokens_if_container_documents_are_less_than, 
        no_above=filter_tokens_if_appeared_percentage_more_than, 
        keep_n=keep_the_first_n_tokens)
    words = [k for k, v in dictionary.token2id.items()]
    return words

  def one_hot_encode_identity(self,words_list,words_dict):
    features = np.zeros((len(words_list),len(words_dict)))
    for i in range(len(words_list)):
      word_i=words_list[i]
      if word_i in words_dict:
        word_index = words_dict.index(word_i)
        features[i,word_index]=1
    return features

  def classify(self, keywords: Tuple):
    # print(keywords)
    keywords = [stemmer.stem(word_i) for word_i in keywords] #Stem words
    verbs_features = self.one_hot_encode_identity([keywords[0]],self.verbs_dict)
    n1_features = self.one_hot_encode_identity([keywords[1]],self.n1_dict)
    p_features = self.one_hot_encode_identity([keywords[2]],self.p_dict)
    n2_features = self.one_hot_encode_identity([keywords[3]],self.n2_dict)

    v_p_pair = [keywords[0],keywords[2]]
    n1_p_pair = [keywords[1],keywords[2]]
    p_n2_pair = [keywords[2],keywords[3]]
    n1_p_n2_pair = [keywords[0],keywords[1],keywords[2]]

    v_p_pair_count = [self.get_large_corpus_count(v_p_pair,self.v_p_pairs_in_corpus)]
    n1_p_pair_count = [self.get_large_corpus_count(n1_p_pair,self.n1_p_pairs_in_corpus)]
    p_n2_pair_count = [self.get_large_corpus_count(p_n2_pair,self.p_n2_pairs_in_corpus)]
    n1_p_n2_pair_count = [self.get_large_corpus_count(n1_p_n2_pair,self.n1_p_n2_pairs_in_corpus)]

    


    test_features = np.hstack((verbs_features,n1_features,p_features,n2_features,
                               np.array(v_p_pair_count).reshape((1,1)),np.array(n1_p_pair_count).reshape((1,1)),np.array(p_n2_pair_count).reshape((1,1)),np.array(n1_p_n2_pair_count).reshape((1,1))))

    test_features = self.min_max_scaler.transform(test_features)
    pred=self.classifier.predict(test_features)
    if pred==1:return 'V'
    else:return 'N'
  
# NaiveBayesClassifier(train_examples)
# print(v_p_pairs_in_corpus)
# print(n1_p_pairs_in_corpus)
# print(p_n2_pairs_in_corpus)
# print(n1_p_n2_pairs_in_corpus)


print(evaluate_classifier(test_examples, SVC2Classifier(train_examples,v_p_pairs_in_corpus,n1_p_pairs_in_corpus,p_n2_pairs_in_corpus,n1_p_n2_pairs_in_corpus)))


### All credit goes to Violet that helped me a lot, I used her code after she explained and showed it to me
# I tried several things before that seamed to not work at all, and my way of counting seamed to be relativelly bad, a lot of computation time, 
# Follows my first attempt (commented)


# import nltk
# from nltk.corpus import wordnet as wn
# import pandas as pd
# import numpy as np
# from nltk.corpus import brown
# from gensim.utils import n2cp
# from gensim import corpora
# from sklearn.svm import SVC
# from nltk.stem.snowball import SnowballStemmer
# from sklearn import preprocessing
# stemmer = SnowballStemmer("english")
# # nltk.download('punkt')
# # nltk.download('averaged_perceptron_tagger')
# # nltk.download('brown')
# # nltk.download('wordnet')
# # nltk.download('universal_tagset')


# df = pd.DataFrame(pp_examples)
# print(df)
# propositionList = df.keywords.tolist()
# v= [v for v,n1,p,n2 in propositionList]
# n1= [n1 for v,n1,p,n2 in propositionList]
# p= [p for v,n1,p,n2 in propositionList]
# n2= [n2 for v,n1,p,n2 in propositionList]
# df= df.assign(v = v)
# df= df.assign(n1= n1)
# df= df.assign(p = p)
# df= df.assign(n2 = n2)
# df = df.assign(len_v= [len(x) for x in df.v.tolist()])
# df = df.assign(len_n1 = [len(x) for x in df.n1.tolist()])
# df = df.assign(len_p = [len(x) for x in df.p.tolist()])
# df = df.assign(len_n2 = [len(x) for x in df.n2.tolist()])

# tagged_sent= brown.tagged_sents()
# sents = brown.sents()


# vp_dist_list = []
# vp_counts = []
# n1p_counts = []
# n1pn2_counts = []
# count = 0

# for v, n1, p, n2 in df.keywords.tolist():
#     vp_counts.append(len([res for res in sents if v in res and p in res]))
#     n1p_counts.append(len([res for res in sents if n1 in res and p in res]))
#     n1pn2_counts.append(len([res for res in sents if n1 in res and p in res]))




# for k,v in df.iterrows():
#     mean_vp_dist = []
#     # for s in sents[0:1000]:
#         isV =0
#         isP = 0
#         for i, (word, tag) in enumerate(s):
#             word= s[i][0] 
#             tag = s[i][1]
#             if str.lower(word) == str.lower(v): isV=i
#             if str.lower(word) == str.lower(p): isP=i

#             mean_vp_dist.append(isV - isP)
#         else: mean_vp_dist .append(0)
#     vp_dist_list.append(np.mean(mean_vp_dist ))
#     # if count %1000 == 0 : print(count)




# df = df.assign(vp_dist= vp_dist_list)
# df = df.assign(vp_count= vp_counts)
# df = df.assign(n1p_count= n1p_counts)
# df = df.assign(n1pn2_count= n1pn2_counts)


# # df = df.drop(columns="keywords")

# transformed_df = df.copy()
# transformed_df.v = [0] * len(transformed_df)
# transformed_df.n1 = [1] * len(transformed_df)
# transformed_df.n2 = [1] * len(transformed_df)
# transformed_df.p = [2] * len(transformed_df)
# transformed_df.to_csv("testdata.csv", sep=",", index=False)


# print(transformed_df)
# print(df)

# wordtags = nltk.ConditionalFreqDist((w.lower(), t) for w, t in brown.tagged_words())
# print(list(wordtags)[:10])
# wsj = brown.tagged_words(tagset='universal')
# wsj = brown.tagged_words()
# print(wsj)

    

    

# cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)
# print(list(cfd2['VBN']))
# print(cfd2.keys())


# from sklearn.model_selection import train_test_split
# from sklearn import svm
# from sklearn import metrics

# X_train, X_test, y_train, y_test = train_test_split(transformed_df.drop(columns="answer"), transformed_df.answer, test_size=0.3,random_state=109) # 70% training and 30% test

# clf = svm.SVC(kernel='linear') # Linear Kernel
# clf.fit(X_train, y_train)
# y_pred = clf.predict(X_test)
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
